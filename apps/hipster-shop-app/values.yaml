components:
  accountingService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      value: cumulative
    - name: KAFKA_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-kafka:9092'
    name: accountingservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 20Mi
    serviceAccount: ""
    useDefault:
      env: true
  adService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: AD_SERVICE_PORT
      value: "8080"
    name: adservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 300Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  cartService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: ASPNETCORE_URLS
      value: http://*:8080
    - name: REDIS_ADDR
      value: '{{ include "otel-demo.name" . }}-redis:6379'
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: CART_SERVICE_PORT
      value: "8080"
    name: cartservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 160Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  checkoutService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: CART_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-cartservice:8080'
    - name: CURRENCY_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-currencyservice:8080'
    - name: PAYMENT_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-paymentservice:8080'
    - name: PRODUCT_CATALOG_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-productcatalogservice:8080'
    - name: SHIPPING_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-shippingservice:8080'
    - name: EMAIL_SERVICE_ADDR
      value: http://{{ include "otel-demo.name" . }}-emailservice:8080
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: CHECKOUT_SERVICE_PORT
      value: "8080"
    - name: KAFKA_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-kafka:9092'
    name: checkoutservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 20Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  currencyService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: PORT
      value: "8080"
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: CURRENCY_SERVICE_PORT
      value: "8080"
    name: currencyservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 20Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  emailService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: APP_ENV
      value: production
    - name: PORT
      value: "8080"
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4318/v1/traces
    - name: EMAIL_SERVICE_PORT
      value: "8080"
    name: emailservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 100Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  featureflagService:
    enabled: false
    env:
    - name: FEATURE_FLAG_GRPC_SERVICE_PORT
      value: "50053"
    - name: FEATURE_FLAG_SERVICE_PORT
      value: "8081"
    - name: OTEL_EXPORTER_OTLP_TRACES_PROTOCOL
      value: grpc
    - name: DATABASE_URL
      value: ecto://ffs:ffs@{{ include "otel-demo.name" . }}-ffspostgres:5432/ffs
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    ports:
    - name: grpc
      value: 50053
    - name: http
      value: 8081
    resources:
      limits:
        memory: 175Mi
    useDefault:
      env: true
  ffsPostgres:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: POSTGRES_DB
      value: ffs
    - name: POSTGRES_PASSWORD
      value: ffs
    - name: POSTGRES_USER
      value: ffs
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    imageOverride:
      repository: postgres
      tag: "14"
    name: ffspostgres
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    ports:
    - name: postgres
      value: 5432
    resources:
      limits:
        memory: 120Mi
    securityContext:
      runAsGroup: 999
      runAsNonRoot: true
      runAsUser: 999
    serviceAccount: ""
    useDefault:
      env: true
  frauddetectionService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      value: cumulative
    - name: KAFKA_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-kafka:9092'
    name: frauddetectionservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 200Mi
    serviceAccount: ""
    useDefault:
      env: true
  frontend:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: FRONTEND_ADDR
      value: :8080
    - name: AD_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-adservice:8080'
    - name: CART_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-cartservice:8080'
    - name: CHECKOUT_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-checkoutservice:8080'
    - name: CURRENCY_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-currencyservice:8080'
    - name: PRODUCT_CATALOG_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-productcatalogservice:8080'
    - name: RECOMMENDATION_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-recommendationservice:8080'
    - name: SHIPPING_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-shippingservice:8080'
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: FRONTEND_PORT
      value: "8080"
    - name: PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
      value: http://localhost:4318/v1/traces
    name: frontend
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 200Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  frontendProxy:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: FRONTEND_PORT
      value: "8080"
    - name: FRONTEND_HOST
      value: '{{ include "otel-demo.name" . }}-frontend'
    - name: FEATURE_FLAG_SERVICE_PORT
      value: "8081"
    - name: FEATURE_FLAG_SERVICE_HOST
      value: '{{ include "otel-demo.name" . }}-featureflagservice'
    - name: LOCUST_WEB_PORT
      value: "8089"
    - name: LOCUST_WEB_HOST
      value: '{{ include "otel-demo.name" . }}-loadgenerator'
    - name: GRAFANA_SERVICE_PORT
      value: "80"
    - name: GRAFANA_SERVICE_HOST
      value: '{{ include "otel-demo.name" . }}-grafana'
    - name: JAEGER_SERVICE_PORT
      value: "16686"
    - name: JAEGER_SERVICE_HOST
      value: '{{ include "otel-demo.name" . }}-jaeger-query'
    - name: OTEL_COLLECTOR_PORT
      value: "4317"
    - name: OTEL_COLLECTOR_HOST
      value: '{{ include "otel-demo.name" . }}-otelcol'
    - name: ENVOY_PORT
      value: "8080"
    name: frontendproxy
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 30Mi
    securityContext:
      runAsGroup: 101
      runAsNonRoot: true
      runAsUser: 101
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  kafka:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: KAFKA_ADVERTISED_LISTENERS
      value: PLAINTEXT://{{ include "otel-demo.name" . }}-kafka:9092
    name: kafka
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    ports:
    - name: plaintext
      value: 9092
    - name: controller
      value: 9093
    resources:
      limits:
        memory: 600Mi
    securityContext:
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: ""
    useDefault:
      env: false
  loadgenerator:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: FRONTEND_ADDR
      value: '{{ include "otel-demo.name" . }}-frontend:8080'
    - name: LOCUST_WEB_PORT
      value: "8089"
    - name: LOCUST_USERS
      value: "10"
    - name: LOCUST_SPAWN_RATE
      value: "1"
    - name: LOCUST_HOST
      value: http://$(FRONTEND_ADDR)
    - name: LOCUST_HEADLESS
      value: "false"
    - name: LOCUST_AUTOSTART
      value: "true"
    - name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION
      value: python
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: LOADGENERATOR_PORT
      value: "8089"
    name: loadgenerator
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 120Mi
    serviceAccount: ""
    servicePort: 8089
    useDefault:
      env: true
  paymentService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: PAYMENT_SERVICE_PORT
      value: "8080"
    name: paymentservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 70Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  productCatalogService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: PRODUCT_CATALOG_SERVICE_PORT
      value: "8080"
    - name: FEATURE_FLAG_GRPC_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-featureflagservice:50053'
    name: productcatalogservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 20Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  quoteService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: OTEL_TRACES_SAMPLER
      value: parentbased_always_on
    - name: OTEL_TRACES_EXPORTER
      value: otlp
    - name: OTEL_EXPORTER_OTLP_TRACES_PROTOCOL
      value: http/protobuf
    - name: OTEL_PHP_TRACES_PROCESSOR
      value: simple
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4318
    - name: QUOTE_SERVICE_PORT
      value: "8080"
    name: quoteservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 40Mi
    securityContext:
      runAsGroup: 33
      runAsNonRoot: true
      runAsUser: 33
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  recommendationService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: OTEL_PYTHON_LOG_CORRELATION
      value: "true"
    - name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION
      value: python
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: FEATURE_FLAG_GRPC_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-featureflagservice:50053'
    - name: RECOMMENDATION_SERVICE_PORT
      value: "8080"
    - name: PRODUCT_CATALOG_SERVICE_ADDR
      value: '{{ include "otel-demo.name" . }}-productcatalogservice:8080'
    name: recommendationservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 500Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
  redis:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    imageOverride:
      repository: redis
      tag: alpine
    name: redis
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    ports:
    - name: redis
      value: 6379
    resources:
      limits:
        memory: 20Mi
    securityContext:
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 999
    serviceAccount: ""
    useDefault:
      env: true
  shippingService:
    Capabilities:
      APIVersions:
      - autoscaling/v2
      - apiextensions.k8s.io/v1
      - v1/Scale
      - storage.k8s.io/v1/CSINode
      - storage.k8s.io/v1/CSIStorageCapacity
      - flowcontrol.apiserver.k8s.io/v1beta1/PriorityLevelConfiguration
      - apps/v1
      - events.k8s.io/v1
      - v1/ReplicationController
      - v1/TokenRequest
      - apps/v1/DaemonSet
      - apps/v1/ReplicaSet
      - networking.k8s.io/v1/NetworkPolicy
      - rbac.authorization.k8s.io/v1/RoleBinding
      - autoscaling/v2beta2
      - certificates.k8s.io/v1
      - apiextensions.k8s.io/v1/CustomResourceDefinition
      - admissionregistration.k8s.io/v1/ValidatingWebhookConfiguration
      - node.k8s.io/v1/RuntimeClass
      - authorization.k8s.io/v1/SelfSubjectRulesReview
      - policy/v1/PodDisruptionBudget
      - networking.k8s.io/v1
      - v1/PodAttachOptions
      - v1/PodTemplate
      - apps/v1/Deployment
      - events.k8s.io/v1/Event
      - v1
      - autoscaling/v1
      - v1/Node
      - v1/PodProxyOptions
      - v1/Secret
      - storage.k8s.io/v1/CSIDriver
      - v1/Event
      - v1/LimitRange
      - v1/ResourceQuota
      - storage.k8s.io/v1/StorageClass
      - flowcontrol.apiserver.k8s.io/v1beta2/PriorityLevelConfiguration
      - authorization.k8s.io/v1
      - node.k8s.io/v1
      - networking.k8s.io/v1/IngressClass
      - scheduling.k8s.io/v1/PriorityClass
      - admissionregistration.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta2
      - v1/Service
      - v1/ServiceProxyOptions
      - apps/v1/ControllerRevision
      - batch/v1/CronJob
      - flowcontrol.apiserver.k8s.io/v1beta1/FlowSchema
      - rbac.authorization.k8s.io/v1
      - v1/PersistentVolumeClaim
      - authorization.k8s.io/v1/SelfSubjectAccessReview
      - rbac.authorization.k8s.io/v1/ClusterRole
      - rbac.authorization.k8s.io/v1/Role
      - flowcontrol.apiserver.k8s.io/v1beta2/FlowSchema
      - coordination.k8s.io/v1
      - v1/ConfigMap
      - authorization.k8s.io/v1/SubjectAccessReview
      - discovery.k8s.io/v1/EndpointSlice
      - apiregistration.k8s.io/v1
      - v1/Namespace
      - autoscaling/v1/HorizontalPodAutoscaler
      - policy/v1
      - v1/PodPortForwardOptions
      - v1/Pod
      - apps/v1/StatefulSet
      - storage.k8s.io/v1/VolumeAttachment
      - storage.k8s.io/v1beta1/CSIStorageCapacity
      - batch/v1
      - v1/ComponentStatus
      - authentication.k8s.io/v1/TokenReview
      - autoscaling/v2/HorizontalPodAutoscaler
      - certificates.k8s.io/v1/CertificateSigningRequest
      - networking.k8s.io/v1/Ingress
      - storage.k8s.io/v1beta1
      - v1/Binding
      - v1/NodeProxyOptions
      - admissionregistration.k8s.io/v1/MutatingWebhookConfiguration
      - coordination.k8s.io/v1/Lease
      - authentication.k8s.io/v1
      - storage.k8s.io/v1
      - v1/PersistentVolume
      - v1/Eviction
      - v1/PodExecOptions
      - v1/ServiceAccount
      - authorization.k8s.io/v1/LocalSubjectAccessReview
      - autoscaling/v2beta2/HorizontalPodAutoscaler
      - scheduling.k8s.io/v1
      - v1/Endpoints
      - batch/v1/Job
      - apiregistration.k8s.io/v1/APIService
      - apps/v1/Scale
      - rbac.authorization.k8s.io/v1/ClusterRoleBinding
      - discovery.k8s.io/v1
      - flowcontrol.apiserver.k8s.io/v1beta1
      HelmVersion:
        git_commit: 50f003e5ee8704ec937a756c646870227d7c8b58
        git_tree_state: clean
        go_version: go1.19.3
        version: v3.10.2
      KubeVersion:
        Major: "1"
        Minor: "25"
        Version: v1.25.3
    Chart:
      IsRoot: true
      apiVersion: v2
      appVersion: 1.2.1
      dependencies:
      - condition: observability.otelcol.enabled
        enabled: true
        name: opentelemetry-collector
        repository: https://open-telemetry.github.io/opentelemetry-helm-charts
        version: 0.40.7
      - condition: observability.jaeger.enabled
        enabled: true
        name: jaeger
        repository: https://jaegertracing.github.io/helm-charts
        version: 0.65.1
      - condition: observability.prometheus.enabled
        enabled: true
        name: prometheus
        repository: https://prometheus-community.github.io/helm-charts
        version: 19.0.1
      - condition: observability.grafana.enabled
        enabled: true
        name: grafana
        repository: https://grafana.github.io/helm-charts
        version: 6.45.1
      description: opentelemetry demo helm chart
      home: https://opentelemetry.io/
      icon: https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png
      maintainers:
      - name: dmitryax
      - name: puckpuck
      - name: tylerhelmuth
      name: opentelemetry-demo
      sources:
      - https://github.com/open-telemetry/opentelemetry-demo
      type: application
      version: 0.15.2
    Release:
      IsInstall: true
      IsUpgrade: false
      Name: my-otel-demo
      Namespace: default
      Revision: 1
      Service: Helm
    Template:
      BasePath: opentelemetry-demo/templates
      Name: opentelemetry-demo/templates/component.yaml
    defaultValues:
      env:
      - name: OTEL_SERVICE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['app.kubernetes.io/component']
      - name: OTEL_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OTEL_K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OTEL_K8S_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
      envOverrides: []
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        repository: ghcr.io/open-telemetry/demo
        tag: ""
      schedulingRules:
        affinity: {}
        nodeSelector: {}
        tolerations: []
      securityContext: {}
    enabled: true
    env:
    - name: PORT
      value: "8080"
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
      value: http://{{ include "otel-demo.name" . }}-otelcol:4317
    - name: SHIPPING_SERVICE_PORT
      value: "8080"
    - name: QUOTE_SERVICE_ADDR
      value: http://{{ include "otel-demo.name" . }}-quoteservice:8080
    name: shippingservice
    observability:
      grafana:
        enabled: true
      jaeger:
        enabled: true
      otelcol:
        enabled: true
      prometheus:
        enabled: true
    resources:
      limits:
        memory: 20Mi
    serviceAccount: ""
    servicePort: 8080
    useDefault:
      env: true
default:
  env:
  - name: OTEL_SERVICE_NAME
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: metadata.labels['app.kubernetes.io/component']
  - name: OTEL_K8S_NAMESPACE
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: metadata.namespace
  - name: OTEL_K8S_NODE_NAME
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: spec.nodeName
  - name: OTEL_K8S_POD_NAME
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: metadata.name
  - name: OTEL_RESOURCE_ATTRIBUTES
    value: service.name=$(OTEL_SERVICE_NAME),k8s.namespace.name=$(OTEL_K8S_NAMESPACE),k8s.node.name=$(OTEL_K8S_NODE_NAME),k8s.pod.name=$(OTEL_K8S_POD_NAME)
  envOverrides: []
  image:
    pullPolicy: IfNotPresent
    pullSecrets: []
    repository: ghcr.io/open-telemetry/demo
    tag: ""
  schedulingRules:
    affinity: {}
    nodeSelector: {}
    tolerations: []
  securityContext: {}
grafana:
  admin:
    existingSecret: ""
    passwordKey: admin-password
    userKey: admin-user
  adminPassword: admin
  adminUser: admin
  affinity: {}
  alerting: {}
  autoscaling:
    enabled: false
  containerSecurityContext: {}
  createConfigmap: true
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - disableDeletion: false
        editable: true
        folder: ""
        name: default
        options:
          path: /var/lib/grafana/dashboards/default
        orgId: 1
        type: file
  dashboards: {}
  dashboardsConfigMaps:
    default: '{{ include "otel-demo.name" . }}-grafana-dashboards'
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - editable: true
        isDefault: true
        name: Prometheus
        type: prometheus
        uid: webstore-metrics
        url: http://{{ include "otel-demo.name" . }}-prometheus-server:9090
      - editable: true
        isDefault: false
        name: Jaeger
        type: jaeger
        uid: webstore-traces
        url: http://{{ include "otel-demo.name" . }}-jaeger-query:16686/jaeger/ui
  deploymentStrategy:
    type: RollingUpdate
  downloadDashboards:
    env: {}
    envFromSecret: ""
    envValueFrom: {}
    resources: {}
    securityContext: {}
  downloadDashboardsImage:
    pullPolicy: IfNotPresent
    repository: curlimages/curl
    sha: ""
    tag: 7.85.0
  enableKubeBackwardCompatibility: false
  enableServiceLinks: true
  env: {}
  envFromConfigMaps: []
  envFromSecret: ""
  envFromSecrets: []
  envRenderSecret: {}
  envValueFrom: {}
  extraConfigmapMounts: []
  extraContainerVolumes: []
  extraContainers: ""
  extraEmptyDirMounts: []
  extraExposePorts: []
  extraInitContainers: []
  extraLabels: {}
  extraObjects: []
  extraSecretMounts: []
  extraVolumeMounts: []
  global:
    imagePullSecrets: []
  grafana.ini:
    analytics:
      check_for_updates: true
    auth:
      disable_login_form: true
    auth.anonymous:
      enabled: true
      org_name: Main Org.
      org_role: Admin
    grafana_net:
      url: https://grafana.net
    log:
      mode: console
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    server:
      domain: '{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ .Values.ingress.hosts
        | first }}{{ else }}''''{{ end }}'
      root_url: '%(protocol)s://%(domain)s:%(http_port)s/grafana'
      serve_from_sub_path: true
  headlessService: false
  hostAliases: []
  image:
    pullPolicy: IfNotPresent
    pullSecrets: []
    repository: grafana/grafana
    sha: ""
    tag: ""
  imageRenderer:
    affinity: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    deploymentStrategy: {}
    enabled: false
    env:
      HTTP_HOST: 0.0.0.0
    grafanaProtocol: http
    grafanaSubPath: ""
    hostAliases: []
    image:
      pullPolicy: Always
      repository: grafana/grafana-image-renderer
      sha: ""
      tag: latest
    networkPolicy:
      limitEgress: false
      limitIngress: true
    nodeSelector: {}
    podPortName: http
    priorityClassName: ""
    replicas: 1
    resources: {}
    revisionHistoryLimit: 10
    securityContext: {}
    service:
      appProtocol: ""
      enabled: true
      port: 8081
      portName: http
      targetPort: 8081
    serviceAccountName: ""
    tolerations: []
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  initChownData:
    enabled: true
    image:
      pullPolicy: IfNotPresent
      repository: busybox
      sha: ""
      tag: 1.31.1
    resources: {}
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
  ldap:
    config: ""
    enabled: false
    existingSecret: ""
  lifecycleHooks: {}
  livenessProbe:
    failureThreshold: 10
    httpGet:
      path: /api/health
      port: 3000
    initialDelaySeconds: 60
    timeoutSeconds: 30
  namespaceOverride: ""
  networkPolicy:
    allowExternal: true
    egress:
      enabled: false
      ports: []
    enabled: false
    explicitNamespacesSelector: {}
    ingress: true
  nodeSelector: {}
  notifiers: {}
  persistence:
    accessModes:
    - ReadWriteOnce
    enabled: false
    extraPvcLabels: {}
    finalizers:
    - kubernetes.io/pvc-protection
    inMemory:
      enabled: false
    size: 10Gi
    type: pvc
  plugins: []
  podDisruptionBudget: {}
  podPortName: grafana
  rbac:
    create: true
    extraClusterRoleRules: []
    extraRoleRules: []
    namespaced: false
    pspEnabled: true
    pspUseAppArmor: true
  readinessProbe:
    httpGet:
      path: /api/health
      port: 3000
  replicas: 1
  resources:
    limits:
      memory: 75Mi
  revisionHistoryLimit: 10
  securityContext:
    fsGroup: 472
    runAsGroup: 472
    runAsUser: 472
  service:
    annotations: {}
    appProtocol: ""
    enabled: true
    labels: {}
    port: 80
    portName: service
    targetPort: 3000
    type: ClusterIP
  serviceAccount:
    autoMount: true
    create: true
    labels: {}
  serviceMonitor:
    enabled: false
    interval: 1m
    labels: {}
    path: /metrics
    relabelings: []
    scheme: http
    scrapeTimeout: 30s
    tlsConfig: {}
  sidecar:
    alerts:
      enabled: false
      env: {}
      label: grafana_alert
      labelValue: ""
      reloadURL: http://localhost:3000/api/admin/provisioning/alerting/reload
      resource: both
      sizeLimit: {}
      skipReload: false
      watchMethod: WATCH
    dashboards:
      SCProvider: true
      enabled: false
      env: {}
      extraMounts: []
      folder: /tmp/dashboards
      label: grafana_dashboard
      labelValue: ""
      provider:
        allowUiUpdates: false
        disableDelete: false
        folder: ""
        foldersFromFilesStructure: false
        name: sidecarProvider
        orgid: 1
        type: file
      resource: both
      sizeLimit: {}
      watchMethod: WATCH
    datasources:
      enabled: false
      env: {}
      initDatasources: false
      label: grafana_datasource
      labelValue: ""
      reloadURL: http://localhost:3000/api/admin/provisioning/datasources/reload
      resource: both
      sizeLimit: {}
      skipReload: false
      watchMethod: WATCH
    enableUniqueFilenames: false
    image:
      repository: quay.io/kiwigrid/k8s-sidecar
      sha: ""
      tag: 1.19.2
    imagePullPolicy: IfNotPresent
    livenessProbe: {}
    notifiers:
      enabled: false
      env: {}
      initNotifiers: false
      label: grafana_notifier
      labelValue: ""
      reloadURL: http://localhost:3000/api/admin/provisioning/notifications/reload
      resource: both
      sizeLimit: {}
      skipReload: false
      watchMethod: WATCH
    plugins:
      enabled: false
      env: {}
      initPlugins: false
      label: grafana_plugin
      labelValue: ""
      reloadURL: http://localhost:3000/api/admin/provisioning/plugins/reload
      resource: both
      sizeLimit: {}
      skipReload: false
      watchMethod: WATCH
    readinessProbe: {}
    resources: {}
    securityContext: {}
  smtp:
    existingSecret: ""
    passwordKey: password
    userKey: user
  testFramework:
    enabled: true
    image: bats/bats
    imagePullPolicy: IfNotPresent
    securityContext: {}
    tag: v1.4.1
  tolerations: []
  topologySpreadConstraints: []
  useStatefulSet: false
jaeger:
  agent:
    affinity: {}
    annotations: {}
    cmdlineParams: {}
    daemonset:
      updateStrategy: {}
      useHostPort: false
    dnsPolicy: ClusterFirst
    enabled: false
    extraConfigmapMounts: []
    extraEnv: []
    extraSecretMounts: []
    image: jaegertracing/jaeger-agent
    imagePullSecrets: []
    initContainers: []
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    podSecurityContext: {}
    priorityClassName: ""
    pullPolicy: IfNotPresent
    resources: {}
    securityContext: {}
    service:
      annotations: {}
      binaryPort: 6832
      compactPort: 6831
      loadBalancerSourceRanges: []
      samplingPort: 5778
      type: ClusterIP
      zipkinThriftPort: 5775
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: false
      create: true
    serviceMonitor:
      additionalLabels: {}
      enabled: false
      metricRelabelings: []
      relabelings: []
    tolerations: []
    useHostNetwork: false
  allInOne:
    args:
    - --memory.max-traces
    - "10000"
    - --query.base-path
    - /jaeger/ui
    enabled: true
    extraEnv: []
    image: jaegertracing/all-in-one
    ingress:
      annotations: {}
      enabled: false
      labels: {}
    nodeSelector: {}
    pullPolicy: IfNotPresent
    resources:
      limits:
        memory: 275Mi
  cassandra:
    affinity: {}
    argsOverrides: []
    backup:
      annotations:
        iam.amazonaws.com/role: cain
      destination: s3://bucket/cassandra
      enabled: false
      env:
      - name: AWS_REGION
        value: us-east-1
      extraArgs: []
      image:
        repository: maorfr/cain
        tag: 0.6.0
      resources:
        limits:
          cpu: 1
          memory: 1Gi
        requests:
          cpu: 1
          memory: 1Gi
      schedule:
      - cron: 0 7 * * *
        keyspace: keyspace1
      - cron: 30 7 * * *
        keyspace: keyspace2
    commandOverrides: []
    config:
      cluster_domain: cluster.local
      cluster_name: jaeger
      cluster_size: 3
      dc_name: dc1
      endpoint_snitch: GossipingPropertyFileSnitch
      heap_new_size: 512M
      max_heap_size: 2048M
      num_tokens: 256
      ports:
        cql: 9042
        thrift: 9160
      rack_name: rack1
      seed_size: 1
      start_rpc: false
    configOverrides: {}
    env: {}
    exporter:
      enabled: false
      image:
        repo: criteord/cassandra_exporter
        tag: 2.0.2
      jvmOpts: ""
      port: 5556
      resources: {}
      serviceMonitor:
        additionalLabels: {}
        enabled: false
    extraContainers: []
    extraVolumeMounts: []
    extraVolumes: []
    global: {}
    hostNetwork: false
    image:
      pullPolicy: IfNotPresent
      repo: cassandra
      tag: 3.11.6
    livenessProbe:
      failureThreshold: 3
      initialDelaySeconds: 90
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 5
    persistence:
      accessMode: ReadWriteOnce
      enabled: false
      size: 10Gi
    podAnnotations: {}
    podDisruptionBudget: {}
    podLabels: {}
    podManagementPolicy: OrderedReady
    podSettings:
      terminationGracePeriodSeconds: 30
    rbac:
      create: true
    readinessProbe:
      address: ${POD_IP}
      failureThreshold: 3
      initialDelaySeconds: 90
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 5
    resources: {}
    securityContext:
      enabled: false
      fsGroup: 999
      runAsUser: 999
    service:
      annotations: ""
      type: ClusterIP
    serviceAccount:
      create: true
    tolerations: []
    updateStrategy:
      type: OnDelete
  collector:
    affinity: {}
    annotations: {}
    autoscaling:
      enabled: false
      maxReplicas: 10
      minReplicas: 2
    cmdlineParams: {}
    dnsPolicy: ClusterFirst
    enabled: false
    extraConfigmapMounts: []
    extraEnv: []
    extraSecretMounts: []
    image: jaegertracing/jaeger-collector
    imagePullSecrets: []
    ingress:
      annotations: {}
      enabled: false
      labels: {}
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    podSecurityContext: {}
    priorityClassName: ""
    pullPolicy: IfNotPresent
    replicaCount: 1
    resources: {}
    securityContext: {}
    service:
      annotations: {}
      clusterIP: ""
      grpc:
        port: 14250
      http:
        port: 14268
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      otlp:
        grpc: {}
        http: {}
      type: ClusterIP
      zipkin: {}
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: false
      create: true
    serviceMonitor:
      additionalLabels: {}
      enabled: false
      metricRelabelings: []
      relabelings: []
    tolerations: []
  common:
    exampleValue: common-chart
    global: {}
  elasticsearch:
    antiAffinity: hard
    antiAffinityTopologyKey: kubernetes.io/hostname
    clusterDeprecationIndexing: "false"
    clusterHealthCheckParams: wait_for_status=green&timeout=1s
    clusterName: elasticsearch
    enableServiceLinks: true
    envFrom: []
    esConfig: {}
    esJavaOpts: ""
    esJvmOptions: {}
    esMajorVersion: ""
    extraContainers: []
    extraEnvs: []
    extraInitContainers: []
    extraVolumeMounts: []
    extraVolumes: []
    fsGroup: ""
    fullnameOverride: ""
    global: {}
    healthNameOverride: ""
    hostAliases: []
    httpPort: 9200
    image: docker.elastic.co/elasticsearch/elasticsearch
    imagePullPolicy: IfNotPresent
    imagePullSecrets: []
    imageTag: 7.17.3
    ingress:
      annotations: {}
      className: nginx
      enabled: false
      hosts:
      - host: chart-example.local
        paths:
        - path: /
      pathtype: ImplementationSpecific
      tls: []
    initResources: {}
    keystore: []
    labels: {}
    lifecycle: {}
    masterService: ""
    maxUnavailable: 1
    minimumMasterNodes: 2
    nameOverride: ""
    networkHost: 0.0.0.0
    networkPolicy:
      http:
        enabled: false
      transport:
        enabled: false
    nodeAffinity: {}
    nodeGroup: master
    nodeSelector: {}
    persistence:
      annotations: {}
      enabled: true
      labels:
        enabled: false
    podAnnotations: {}
    podManagementPolicy: Parallel
    podSecurityContext:
      fsGroup: 1000
      runAsUser: 1000
    podSecurityPolicy:
      create: false
      name: ""
      spec:
        fsGroup:
          rule: RunAsAny
        privileged: true
        runAsUser:
          rule: RunAsAny
        seLinux:
          rule: RunAsAny
        supplementalGroups:
          rule: RunAsAny
        volumes:
        - secret
        - configMap
        - persistentVolumeClaim
        - emptyDir
    priorityClassName: ""
    protocol: http
    rbac:
      automountToken: true
      create: false
      serviceAccountAnnotations: {}
      serviceAccountName: ""
    readinessProbe:
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 3
      timeoutSeconds: 5
    replicas: 3
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    roles:
      data: "true"
      ingest: "true"
      master: "true"
      ml: "true"
      remote_cluster_client: "true"
    schedulerName: ""
    secretMounts: []
    securityContext:
      capabilities:
        drop:
        - ALL
      runAsNonRoot: true
      runAsUser: 1000
    service:
      annotations: {}
      enabled: true
      externalTrafficPolicy: ""
      httpPortName: http
      labels: {}
      labelsHeadless: {}
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      nodePort: ""
      publishNotReadyAddresses: false
      transportPortName: transport
      type: ClusterIP
    sysctlInitContainer:
      enabled: true
    sysctlVmMaxMapCount: 262144
    terminationGracePeriod: 120
    tests:
      enabled: true
    tolerations: []
    transportPort: 9300
    updateStrategy: RollingUpdate
    volumeClaimTemplate:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 30Gi
  esIndexCleaner:
    affinity: {}
    annotations: {}
    cmdlineParams: {}
    concurrencyPolicy: Forbid
    enabled: false
    extraConfigmapMounts: []
    extraEnv: []
    extraSecretMounts: []
    failedJobsHistoryLimit: 3
    image: jaegertracing/jaeger-es-index-cleaner
    imagePullSecrets: []
    nodeSelector: {}
    numberOfDays: 7
    podAnnotations: {}
    podLabels: {}
    podSecurityContext:
      runAsUser: 1000
    pullPolicy: Always
    resources: {}
    schedule: 55 23 * * *
    securityContext:
      runAsUser: 1000
    serviceAccount:
      automountServiceAccountToken: false
      create: true
    successfulJobsHistoryLimit: 3
    tolerations: []
  esLookback:
    affinity: {}
    annotations: {}
    cmdlineParams: {}
    concurrencyPolicy: Forbid
    enabled: false
    extraConfigmapMounts: []
    extraEnv:
    - name: UNIT
      value: days
    - name: UNIT_COUNT
      value: "7"
    extraSecretMounts: []
    failedJobsHistoryLimit: 3
    image: jaegertracing/jaeger-es-rollover
    imagePullSecrets: []
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    podSecurityContext:
      runAsUser: 1000
    pullPolicy: Always
    resources: {}
    schedule: 5 0 * * *
    securityContext: {}
    serviceAccount:
      automountServiceAccountToken: false
      create: true
    successfulJobsHistoryLimit: 3
    tag: latest
    tolerations: []
  esRollover:
    affinity: {}
    annotations: {}
    cmdlineParams: {}
    concurrencyPolicy: Forbid
    enabled: false
    extraConfigmapMounts: []
    extraEnv:
    - name: CONDITIONS
      value: '{"max_age": "1d"}'
    extraSecretMounts: []
    failedJobsHistoryLimit: 3
    image: jaegertracing/jaeger-es-rollover
    imagePullSecrets: []
    initHook:
      annotations: {}
      extraEnv: []
      podAnnotations: {}
      podLabels: {}
      ttlSecondsAfterFinished: 120
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    podSecurityContext:
      runAsUser: 1000
    pullPolicy: Always
    resources: {}
    schedule: 10 0 * * *
    securityContext: {}
    serviceAccount:
      automountServiceAccountToken: false
      create: true
    successfulJobsHistoryLimit: 3
    tag: latest
    tolerations: []
  extraObjects: []
  fullnameOverride: ""
  global: {}
  hotrod:
    affinity: {}
    enabled: false
    image:
      pullPolicy: Always
      pullSecrets: []
      repository: jaegertracing/example-hotrod
    ingress:
      annotations: {}
      enabled: false
      hosts:
      - chart-example.local
    nodeSelector: {}
    podSecurityContext: {}
    replicaCount: 1
    resources: {}
    securityContext: {}
    service:
      annotations: {}
      loadBalancerSourceRanges: []
      name: hotrod
      port: 80
      type: ClusterIP
    serviceAccount:
      automountServiceAccountToken: false
      create: true
    tolerations: []
    tracing:
      port: 6831
  ingester:
    affinity: {}
    annotations: {}
    autoscaling:
      enabled: false
      maxReplicas: 10
      minReplicas: 2
    cmdlineParams: {}
    dnsPolicy: ClusterFirst
    enabled: false
    extraConfigmapMounts: []
    extraSecretMounts: []
    image: jaegertracing/jaeger-ingester
    imagePullSecrets: []
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    podSecurityContext: {}
    pullPolicy: IfNotPresent
    replicaCount: 1
    resources: {}
    securityContext: {}
    service:
      annotations: {}
      loadBalancerSourceRanges: []
      type: ClusterIP
    serviceAccount:
      automountServiceAccountToken: false
      create: true
    serviceMonitor:
      additionalLabels: {}
      enabled: false
      metricRelabelings: []
      relabelings: []
    tolerations: []
  kafka:
    advertisedListeners: []
    affinity: {}
    allowEveryoneIfNoAclFound: true
    allowPlaintextListener: true
    args: []
    auth:
      clientProtocol: plaintext
      externalClientProtocol: ""
      interBrokerProtocol: plaintext
      sasl:
        interBrokerMechanism: plain
        jaas:
          clientPasswords: []
          clientUsers:
          - user
          existingSecret: ""
          interBrokerPassword: ""
          interBrokerUser: admin
          zookeeperPassword: ""
          zookeeperUser: ""
        mechanisms: plain,scram-sha-256,scram-sha-512
      tls:
        autoGenerated: false
        endpointIdentificationAlgorithm: https
        existingSecret: ""
        existingSecrets: []
        jksKeystoreSAN: ""
        jksTruststore: ""
        jksTruststoreSecret: ""
        password: ""
        pemChainIncluded: false
        type: jks
      zookeeper:
        tls:
          enabled: false
          existingSecret: ""
          existingSecretKeystoreKey: zookeeper.keystore.jks
          existingSecretTruststoreKey: zookeeper.truststore.jks
          passwordsSecret: ""
          passwordsSecretKeystoreKey: keystore-password
          passwordsSecretTruststoreKey: truststore-password
          type: jks
          verifyHostname: true
    authorizerClassName: ""
    autoCreateTopicsEnable: true
    clusterDomain: cluster.local
    command:
    - /scripts/setup.sh
    common:
      exampleValue: common-chart
      global:
        imagePullSecrets: []
        imageRegistry: ""
        storageClass: ""
    commonAnnotations: {}
    commonLabels: {}
    config: ""
    containerPorts:
      client: 9092
      external: 9094
      internal: 9093
    containerSecurityContext:
      allowPrivilegeEscalation: false
      enabled: true
      runAsNonRoot: true
      runAsUser: 1001
    customLivenessProbe: {}
    customReadinessProbe: {}
    customStartupProbe: {}
    defaultReplicationFactor: 1
    deleteTopicEnable: false
    diagnosticMode:
      args:
      - infinity
      command:
      - sleep
      enabled: false
    existingConfigmap: ""
    existingLog4jConfigMap: ""
    externalAccess:
      autoDiscovery:
        enabled: false
        image:
          digest: ""
          pullPolicy: IfNotPresent
          pullSecrets: []
          registry: docker.io
          repository: bitnami/kubectl
          tag: 1.25.1-debian-11-r1
        resources:
          limits: {}
          requests: {}
      enabled: false
      service:
        annotations: {}
        domain: ""
        extraPorts: []
        labels: {}
        loadBalancerAnnotations: []
        loadBalancerIPs: []
        loadBalancerNames: []
        loadBalancerSourceRanges: []
        nodePorts: []
        ports:
          external: 9094
        type: LoadBalancer
        useHostIPs: false
        usePodIPs: false
    externalZookeeper:
      servers: []
    extraDeploy: []
    extraEnvVars: []
    extraEnvVarsCM: ""
    extraEnvVarsSecret: ""
    extraVolumeMounts: []
    extraVolumes: []
    fullnameOverride: ""
    global:
      imagePullSecrets: []
      imageRegistry: ""
      storageClass: ""
    heapOpts: -Xmx1024m -Xms1024m
    hostAliases: []
    hostIPC: false
    hostNetwork: false
    image:
      debug: false
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/kafka
      tag: 3.2.3-debian-11-r1
    initContainers: []
    interBrokerListenerName: INTERNAL
    kubeVersion: ""
    lifecycleHooks: {}
    listenerSecurityProtocolMap: ""
    listeners: []
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    log4j: ""
    logFlushIntervalMessages: _10000
    logFlushIntervalMs: 1000
    logPersistence:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enabled: false
      existingClaim: ""
      mountPath: /opt/bitnami/kafka/logs
      selector: {}
      size: 8Gi
      storageClass: ""
    logRetentionBytes: _1073741824
    logRetentionCheckIntervalMs: 300000
    logRetentionHours: 168
    logSegmentBytes: _1073741824
    logsDirs: /bitnami/kafka/data
    maxMessageBytes: _1000012
    metrics:
      jmx:
        config: |-
          jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi
          lowercaseOutputName: true
          lowercaseOutputLabelNames: true
          ssl: false
          {{- if .Values.metrics.jmx.whitelistObjectNames }}
          whitelistObjectNames: ["{{ join "\",\"" .Values.metrics.jmx.whitelistObjectNames }}"]
          {{- end }}
        containerPorts:
          metrics: 5556
        containerSecurityContext:
          enabled: true
          runAsNonRoot: true
          runAsUser: 1001
        enabled: false
        existingConfigmap: ""
        extraRules: ""
        image:
          digest: ""
          pullPolicy: IfNotPresent
          pullSecrets: []
          registry: docker.io
          repository: bitnami/jmx-exporter
          tag: 0.17.1-debian-11-r3
        resources:
          limits: {}
          requests: {}
        service:
          annotations:
            prometheus.io/path: /
            prometheus.io/port: '{{ .Values.metrics.jmx.service.ports.metrics }}'
            prometheus.io/scrape: "true"
          clusterIP: ""
          ports:
            metrics: 5556
          sessionAffinity: None
        whitelistObjectNames:
        - kafka.controller:*
        - kafka.server:*
        - java.lang:*
        - kafka.network:*
        - kafka.log:*
      kafka:
        affinity: {}
        args: []
        certificatesSecret: ""
        command: []
        containerPorts:
          metrics: 9308
        containerSecurityContext:
          enabled: true
          runAsNonRoot: true
          runAsUser: 1001
        enabled: false
        extraFlags: {}
        extraVolumeMounts: []
        extraVolumes: []
        hostAliases: []
        image:
          digest: ""
          pullPolicy: IfNotPresent
          pullSecrets: []
          registry: docker.io
          repository: bitnami/kafka-exporter
          tag: 1.6.0-debian-11-r10
        initContainers: []
        nodeAffinityPreset:
          key: ""
          type: ""
          values: []
        nodeSelector: {}
        podAffinityPreset: ""
        podAnnotations: {}
        podAntiAffinityPreset: soft
        podLabels: {}
        podSecurityContext:
          enabled: true
          fsGroup: 1001
        priorityClassName: ""
        resources:
          limits: {}
          requests: {}
        schedulerName: ""
        service:
          annotations:
            prometheus.io/path: /metrics
            prometheus.io/port: '{{ .Values.metrics.kafka.service.ports.metrics }}'
            prometheus.io/scrape: "true"
          clusterIP: ""
          ports:
            metrics: 9308
          sessionAffinity: None
        serviceAccount:
          automountServiceAccountToken: true
          create: true
          name: ""
        sidecars: []
        tlsCaCert: ca-file
        tlsCaSecret: ""
        tlsCert: cert-file
        tlsKey: key-file
        tolerations: []
        topologySpreadConstraints: []
      prometheusRule:
        enabled: false
        groups: []
        labels: {}
        namespace: ""
      serviceMonitor:
        enabled: false
        honorLabels: false
        interval: ""
        jobLabel: ""
        labels: {}
        metricRelabelings: []
        namespace: ""
        relabelings: []
        scrapeTimeout: ""
        selector: {}
    minBrokerId: 0
    nameOverride: ""
    networkPolicy:
      allowExternal: true
      egressRules:
        customRules: []
      enabled: false
      explicitNamespacesSelector: {}
      externalAccess:
        from: []
    nodeAffinityPreset:
      key: ""
      type: ""
      values: []
    nodeSelector: {}
    numIoThreads: 8
    numNetworkThreads: 3
    numPartitions: 1
    numRecoveryThreadsPerDataDir: 1
    offsetsTopicReplicationFactor: 1
    pdb:
      create: false
      maxUnavailable: 1
      minAvailable: ""
    persistence:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enabled: true
      existingClaim: ""
      labels: {}
      mountPath: /bitnami/kafka
      selector: {}
      size: 8Gi
      storageClass: ""
    podAffinityPreset: ""
    podAnnotations: {}
    podAntiAffinityPreset: soft
    podLabels: {}
    podManagementPolicy: Parallel
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    priorityClassName: ""
    provisioning:
      args: []
      auth:
        tls:
          caCert: ca.crt
          cert: tls.crt
          certificatesSecret: ""
          key: tls.key
          keyPassword: ""
          keyPasswordSecretKey: key-password
          keystore: keystore.jks
          keystorePassword: ""
          keystorePasswordSecretKey: keystore-password
          passwordsSecret: ""
          truststore: truststore.jks
          truststorePassword: ""
          truststorePasswordSecretKey: truststore-password
          type: jks
      command: []
      containerSecurityContext:
        enabled: true
        runAsNonRoot: true
        runAsUser: 1001
      enabled: false
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      extraProvisioningCommands: []
      extraVolumeMounts: []
      extraVolumes: []
      initContainers: []
      numPartitions: 1
      parallel: 1
      podAnnotations: {}
      podLabels: {}
      podSecurityContext:
        enabled: true
        fsGroup: 1001
      postScript: ""
      preScript: ""
      replicationFactor: 1
      resources:
        limits: {}
        requests: {}
      schedulerName: ""
      sidecars: []
      tolerations: []
      topics: []
      waitForKafka: true
    rbac:
      create: false
    readinessProbe:
      enabled: true
      failureThreshold: 6
      initialDelaySeconds: 5
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    replicaCount: 1
    resources:
      limits: {}
      requests: {}
    schedulerName: ""
    service:
      annotations: {}
      clusterIP: ""
      externalTrafficPolicy: Cluster
      extraPorts: []
      headless:
        annotations: {}
        labels: {}
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      nodePorts:
        client: ""
        external: ""
      ports:
        client: 9092
        external: 9094
        internal: 9093
      sessionAffinity: None
      sessionAffinityConfig: {}
      type: ClusterIP
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: true
      create: true
      name: ""
    sidecars: []
    socketReceiveBufferBytes: 102400
    socketRequestMaxBytes: _104857600
    socketSendBufferBytes: 102400
    startupProbe:
      enabled: false
      failureThreshold: 15
      initialDelaySeconds: 30
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    superUsers: User:admin
    terminationGracePeriodSeconds: ""
    tolerations: []
    topologySpreadConstraints: []
    transactionStateLogMinIsr: 1
    transactionStateLogReplicationFactor: 1
    updateStrategy:
      rollingUpdate: {}
      type: RollingUpdate
    volumePermissions:
      containerSecurityContext:
        runAsUser: 0
      enabled: false
      image:
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/bitnami-shell
        tag: 11-debian-11-r37
      resources:
        limits: {}
        requests: {}
    zookeeper:
      affinity: {}
      args: []
      auth:
        client:
          clientPassword: ""
          clientUser: ""
          enabled: false
          existingSecret: ""
          serverPasswords: ""
          serverUsers: ""
        quorum:
          enabled: false
          existingSecret: ""
          learnerPassword: ""
          learnerUser: ""
          serverPasswords: ""
          serverUsers: ""
      autopurge:
        purgeInterval: 0
        snapRetainCount: 3
      clusterDomain: cluster.local
      command:
      - /scripts/setup.sh
      common:
        exampleValue: common-chart
        global:
          imagePullSecrets: []
          imageRegistry: ""
          storageClass: ""
      commonAnnotations: {}
      commonLabels: {}
      configuration: ""
      containerPorts:
        client: 2181
        election: 3888
        follower: 2888
        tls: 3181
      containerSecurityContext:
        allowPrivilegeEscalation: false
        enabled: true
        runAsNonRoot: true
        runAsUser: 1001
      customLivenessProbe: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      dataLogDir: ""
      diagnosticMode:
        args:
        - infinity
        command:
        - sleep
        enabled: false
      enabled: true
      existingConfigmap: ""
      extraDeploy: []
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      extraVolumeMounts: []
      extraVolumes: []
      fourlwCommandsWhitelist: srvr, mntr, ruok
      fullnameOverride: ""
      global:
        imagePullSecrets: []
        imageRegistry: ""
        storageClass: ""
      heapSize: 1024
      hostAliases: []
      image:
        debug: false
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/zookeeper
        tag: 3.8.0-debian-11-r36
      initContainers: []
      initLimit: 10
      jvmFlags: ""
      kubeVersion: ""
      lifecycleHooks: {}
      listenOnAllIPs: false
      livenessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        probeCommandTimeout: 2
        successThreshold: 1
        timeoutSeconds: 5
      logLevel: ERROR
      maxClientCnxns: 60
      maxSessionTimeout: 40000
      metrics:
        containerPort: 9141
        enabled: false
        prometheusRule:
          additionalLabels: {}
          enabled: false
          namespace: ""
          rules: []
        service:
          annotations:
            prometheus.io/path: /metrics
            prometheus.io/port: '{{ .Values.metrics.service.port }}'
            prometheus.io/scrape: "true"
          port: 9141
          type: ClusterIP
        serviceMonitor:
          additionalLabels: {}
          enabled: false
          honorLabels: false
          interval: ""
          jobLabel: ""
          metricRelabelings: []
          namespace: ""
          relabelings: []
          scrapeTimeout: ""
          selector: {}
      minServerId: 1
      nameOverride: ""
      namespaceOverride: ""
      networkPolicy:
        allowExternal: true
        enabled: false
      nodeAffinityPreset:
        key: ""
        type: ""
        values: []
      nodeSelector: {}
      pdb:
        create: false
        maxUnavailable: 1
        minAvailable: ""
      persistence:
        accessModes:
        - ReadWriteOnce
        annotations: {}
        dataLogDir:
          existingClaim: ""
          selector: {}
          size: 8Gi
        enabled: true
        existingClaim: ""
        selector: {}
        size: 8Gi
        storageClass: ""
      podAffinityPreset: ""
      podAnnotations: {}
      podAntiAffinityPreset: soft
      podLabels: {}
      podManagementPolicy: Parallel
      podSecurityContext:
        enabled: true
        fsGroup: 1001
      preAllocSize: 65536
      priorityClassName: ""
      readinessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        probeCommandTimeout: 2
        successThreshold: 1
        timeoutSeconds: 5
      replicaCount: 1
      resources:
        limits: {}
        requests:
          cpu: 250m
          memory: 256Mi
      schedulerName: ""
      service:
        annotations: {}
        clusterIP: ""
        disableBaseClientPort: false
        externalTrafficPolicy: Cluster
        extraPorts: []
        headless:
          annotations: {}
          publishNotReadyAddresses: true
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        nodePorts:
          client: ""
          tls: ""
        ports:
          client: 2181
          election: 3888
          follower: 2888
          tls: 3181
        sessionAffinity: None
        sessionAffinityConfig: {}
        type: ClusterIP
      serviceAccount:
        annotations: {}
        automountServiceAccountToken: true
        create: true
        name: ""
      sidecars: []
      snapCount: 100000
      startupProbe:
        enabled: false
        failureThreshold: 15
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      syncLimit: 5
      tickTime: 2000
      tls:
        client:
          auth: none
          autoGenerated: false
          enabled: false
          existingSecret: ""
          existingSecretKeystoreKey: ""
          existingSecretTruststoreKey: ""
          keystorePassword: ""
          keystorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.keystore.jks
          passwordsSecretKeystoreKey: ""
          passwordsSecretName: ""
          passwordsSecretTruststoreKey: ""
          truststorePassword: ""
          truststorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.truststore.jks
        quorum:
          auth: none
          autoGenerated: false
          enabled: false
          existingSecret: ""
          existingSecretKeystoreKey: ""
          existingSecretTruststoreKey: ""
          keystorePassword: ""
          keystorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.keystore.jks
          passwordsSecretKeystoreKey: ""
          passwordsSecretName: ""
          passwordsSecretTruststoreKey: ""
          truststorePassword: ""
          truststorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.truststore.jks
        resources:
          limits: {}
          requests: {}
      tolerations: []
      topologySpreadConstraints: []
      updateStrategy:
        rollingUpdate: {}
        type: RollingUpdate
      volumePermissions:
        containerSecurityContext:
          runAsUser: 0
        enabled: false
        image:
          digest: ""
          pullPolicy: IfNotPresent
          pullSecrets: []
          registry: docker.io
          repository: bitnami/bitnami-shell
          tag: 11-debian-11-r35
        resources:
          limits: {}
          requests: {}
    zookeeperChrootPath: ""
    zookeeperConnectionTimeoutMs: 6000
  nameOverride: ""
  provisionDataStore:
    cassandra: false
    elasticsearch: false
    kafka: false
  query:
    affinity: {}
    agentSidecar:
      enabled: true
    annotations: {}
    cmdlineParams: {}
    dnsPolicy: ClusterFirst
    enabled: false
    extraConfigmapMounts: []
    extraEnv: []
    extraVolumes: []
    image: jaegertracing/jaeger-query
    imagePullSecrets: []
    ingress:
      annotations: {}
      enabled: false
      health:
        exposed: false
      labels: {}
    nodeSelector: {}
    oAuthSidecar:
      args: []
      containerPort: 4180
      enabled: false
      extraConfigmapMounts: []
      extraEnv: []
      extraSecretMounts: []
      image: quay.io/oauth2-proxy/oauth2-proxy:v7.1.0
      pullPolicy: IfNotPresent
    podAnnotations: {}
    podLabels: {}
    podSecurityContext: {}
    priorityClassName: ""
    pullPolicy: IfNotPresent
    replicaCount: 1
    resources: {}
    securityContext: {}
    service:
      annotations: {}
      loadBalancerSourceRanges: []
      port: 80
      type: ClusterIP
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: false
      create: true
    serviceMonitor:
      additionalLabels: {}
      enabled: false
      metricRelabelings: []
      relabelings: []
    sidecars: []
    tolerations: []
  schema:
    activeDeadlineSeconds: 300
    annotations: {}
    extraEnv: []
    image: jaegertracing/jaeger-cassandra-schema
    imagePullSecrets: []
    podAnnotations: {}
    podLabels: {}
    podSecurityContext: {}
    pullPolicy: IfNotPresent
    resources: {}
    securityContext: {}
    serviceAccount:
      automountServiceAccountToken: true
      create: true
  spark:
    affinity: {}
    annotations: {}
    cmdlineParams: {}
    concurrencyPolicy: Forbid
    enabled: false
    extraConfigmapMounts: []
    extraEnv: []
    extraSecretMounts: []
    failedJobsHistoryLimit: 5
    image: jaegertracing/spark-dependencies
    imagePullSecrets: []
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    pullPolicy: Always
    resources: {}
    schedule: 49 23 * * *
    serviceAccount:
      automountServiceAccountToken: false
      create: true
    successfulJobsHistoryLimit: 5
    tag: latest
    tolerations: []
  storage:
    cassandra:
      cmdlineParams: {}
      extraEnv: []
      host: cassandra
      keyspace: jaeger_v1_test
      password: password
      port: 9042
      tls:
        enabled: false
        secretName: cassandra-tls-secret
      usePassword: true
      user: user
    elasticsearch:
      cmdlineParams: {}
      extraEnv: []
      host: elasticsearch-master
      nodesWanOnly: false
      password: changeme
      port: 9200
      scheme: http
      usePassword: true
      user: elastic
    grpcPlugin:
      extraEnv: []
    kafka:
      authentication: none
      brokers:
      - kafka:9092
      extraEnv: []
      topic: jaeger_v1_test
    type: none
  tag: ""
observability:
  grafana:
    enabled: true
  jaeger:
    enabled: true
  otelcol:
    enabled: true
  prometheus:
    enabled: true
opentelemetry-collector:
  affinity: {}
  annotations: {}
  autoscaling:
    enabled: false
    maxReplicas: 10
    minReplicas: 1
    targetCPUUtilizationPercentage: 80
  clusterRole:
    annotations: {}
    clusterRoleBinding:
      annotations: {}
      name: ""
    create: false
    name: ""
    rules: []
  command:
    extraArgs: []
    name: otelcol-contrib
  config:
    exporters:
      logging: {}
      otlp:
        endpoint: '{{ include "otel-demo.name" . }}-jaeger-collector:4317'
        tls:
          insecure: true
      prometheus:
        endpoint: 0.0.0.0:9464
    extensions:
      health_check: {}
      memory_ballast: {}
    processors:
      batch: {}
      memory_limiter: null
      spanmetrics:
        metrics_exporter: prometheus
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_compact:
            endpoint: 0.0.0.0:6831
          thrift_http:
            endpoint: 0.0.0.0:14268
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            cors:
              allowed_origins:
              - http://*
              - https://*
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - ${MY_POD_IP}:8888
      zipkin:
        endpoint: 0.0.0.0:9411
    service:
      extensions:
      - health_check
      - memory_ballast
      pipelines:
        logs:
          exporters:
          - logging
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
        metrics:
          exporters:
          - prometheus
          - logging
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
          - prometheus
        traces:
          exporters:
          - otlp
          - logging
          processors:
          - memory_limiter
          - spanmetrics
          - batch
          receivers:
          - otlp
          - jaeger
          - zipkin
      telemetry:
        metrics:
          address: 0.0.0.0:8888
  configMap:
    create: true
  dnsPolicy: ""
  extraContainers: []
  extraEnvs: []
  extraVolumeMounts: []
  extraVolumes: []
  fullnameOverride: ""
  global: {}
  hostNetwork: false
  image:
    pullPolicy: IfNotPresent
    repository: otel/opentelemetry-collector-contrib
    tag: ""
  imagePullSecrets: []
  ingress:
    additionalIngresses: []
    enabled: false
  initContainers: []
  lifecycleHooks: {}
  mode: deployment
  nameOverride: otelcol
  nodeSelector: {}
  podAnnotations:
    opentelemetry_community_demo: "true"
    prometheus.io/port: "9464"
    prometheus.io/scrape: "true"
  podDisruptionBudget:
    enabled: false
  podLabels: {}
  podMonitor:
    enabled: false
    extraLabels: {}
    metricsEndpoints:
    - port: metrics
  podSecurityContext: {}
  ports:
    jaeger-compact:
      containerPort: 6831
      enabled: true
      hostPort: 6831
      protocol: UDP
      servicePort: 6831
    jaeger-grpc:
      containerPort: 14250
      enabled: true
      hostPort: 14250
      protocol: TCP
      servicePort: 14250
    jaeger-thrift:
      containerPort: 14268
      enabled: true
      hostPort: 14268
      protocol: TCP
      servicePort: 14268
    metrics:
      containerPort: 8888
      enabled: true
      protocol: TCP
      servicePort: 8888
    otlp:
      containerPort: 4317
      enabled: true
      hostPort: 4317
      protocol: TCP
      servicePort: 4317
    otlp-http:
      containerPort: 4318
      enabled: true
      hostPort: 4318
      protocol: TCP
      servicePort: 4318
    prometheus:
      containerPort: 9464
      enabled: true
      protocol: TCP
      servicePort: 9464
    zipkin:
      containerPort: 9411
      enabled: true
      hostPort: 9411
      protocol: TCP
      servicePort: 9411
  presets:
    clusterMetrics:
      enabled: false
    hostMetrics:
      enabled: false
    kubeletMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: false
    logsCollection:
      enabled: false
      includeCollectorLogs: false
      storeCheckpoints: false
  priorityClassName: ""
  prometheusRule:
    defaultRules:
      enabled: false
    enabled: false
    extraLabels: {}
    groups: []
  replicaCount: 1
  resources:
    limits:
      cpu: 256m
      memory: 125Mi
  rollout:
    rollingUpdate: {}
    strategy: RollingUpdate
  securityContext: {}
  service:
    annotations: {}
    type: ClusterIP
  serviceAccount:
    annotations: {}
    create: true
    name: ""
  serviceMonitor:
    enabled: false
    extraLabels: {}
    metricsEndpoints:
    - port: metrics
  statefulset:
    podManagementPolicy: Parallel
    volumeClaimTemplates: []
  tolerations: []
prometheus:
  alertRelabelConfigs: null
  alertmanager:
    additionalPeers: []
    affinity: {}
    command: []
    config:
      global: {}
      receivers:
      - name: default-receiver
      route:
        group_interval: 5m
        group_wait: 10s
        receiver: default-receiver
        repeat_interval: 3h
      templates:
      - /etc/alertmanager/*.tmpl
    configmapReload:
      enabled: false
      image:
        pullPolicy: IfNotPresent
        repository: jimmidyson/configmap-reload
        tag: v0.8.0
      name: configmap-reload
      resources: {}
    dnsConfig: {}
    enabled: false
    extraArgs: {}
    extraSecretMounts: []
    fullnameOverride: ""
    global: {}
    image:
      pullPolicy: IfNotPresent
      repository: quay.io/prometheus/alertmanager
      tag: ""
    imagePullSecrets: []
    ingress:
      annotations: {}
      className: ""
      enabled: false
      hosts:
      - host: alertmanager.domain.com
        paths:
        - path: /
          pathType: ImplementationSpecific
      tls: []
    livenessProbe:
      httpGet:
        path: /
        port: http
    nameOverride: ""
    nodeSelector: {}
    persistence:
      accessModes:
      - ReadWriteOnce
      enabled: true
      size: 2Gi
    podAnnotations: {}
    podAntiAffinity: ""
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    podDisruptionBudget: {}
    podLabels: {}
    podSecurityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    readinessProbe:
      httpGet:
        path: /
        port: http
    replicaCount: 1
    resources: {}
    securityContext:
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    service:
      annotations: {}
      clusterPort: 9094
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      port: 9093
      type: ClusterIP
    serviceAccount:
      annotations: {}
      create: true
    statefulSet:
      annotations: {}
    templates: {}
    tolerations: []
    topologySpreadConstraints: []
  configmapReload:
    prometheus:
      containerSecurityContext: {}
      enabled: false
      extraArgs: {}
      extraConfigmapMounts: []
      extraVolumeDirs: []
      image:
        pullPolicy: IfNotPresent
        repository: jimmidyson/configmap-reload
        tag: v0.8.0
      name: configmap-reload
      resources: {}
  extraManifests: []
  extraScrapeConfigs: null
  forceNamespace: null
  global: {}
  imagePullSecrets: null
  kube-state-metrics:
    affinity: {}
    annotations: {}
    autosharding:
      enabled: false
    collectors:
    - certificatesigningrequests
    - configmaps
    - cronjobs
    - daemonsets
    - deployments
    - endpoints
    - horizontalpodautoscalers
    - ingresses
    - jobs
    - leases
    - limitranges
    - mutatingwebhookconfigurations
    - namespaces
    - networkpolicies
    - nodes
    - persistentvolumeclaims
    - persistentvolumes
    - poddisruptionbudgets
    - pods
    - replicasets
    - replicationcontrollers
    - resourcequotas
    - secrets
    - services
    - statefulsets
    - storageclasses
    - validatingwebhookconfigurations
    - volumeattachments
    containerSecurityContext: {}
    customLabels: {}
    enabled: false
    extraArgs: []
    global: {}
    hostNetwork: false
    image:
      pullPolicy: IfNotPresent
      repository: registry.k8s.io/kube-state-metrics/kube-state-metrics
      sha: ""
      tag: v2.7.0
    imagePullSecrets: []
    kubeTargetVersionOverride: ""
    kubeconfig:
      enabled: false
    metricAllowlist: []
    metricAnnotationsAllowList: []
    metricDenylist: []
    metricLabelsAllowlist: []
    namespaceOverride: ""
    namespaces: ""
    namespacesDenylist: ""
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget: {}
    podSecurityPolicy:
      additionalVolumes: []
      annotations: {}
      enabled: false
    prometheus:
      monitor:
        additionalLabels: {}
        enabled: false
        honorLabels: false
        interval: ""
        jobLabel: ""
        labelLimit: 0
        labelNameLengthLimit: 0
        labelValueLengthLimit: 0
        metricRelabelings: []
        namespace: ""
        proxyUrl: ""
        relabelings: []
        sampleLimit: 0
        scheme: ""
        scrapeTimeout: ""
        selectorOverride: {}
        targetLimit: 0
        tlsConfig: {}
    prometheusScrape: true
    rbac:
      create: true
      extraRules: []
      useClusterRole: true
    releaseLabel: false
    releaseNamespace: false
    replicas: 1
    resources: {}
    securityContext:
      enabled: true
      fsGroup: 65534
      runAsGroup: 65534
      runAsUser: 65534
    selfMonitor:
      enabled: false
    service:
      annotations: {}
      clusterIP: ""
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      nodePort: 0
      port: 8080
      type: ClusterIP
    serviceAccount:
      annotations: {}
      create: true
      imagePullSecrets: []
    tolerations: []
    topologySpreadConstraints: []
    verticalPodAutoscaler:
      controlledResources: []
      enabled: false
      maxAllowed: {}
      minAllowed: {}
    volumeMounts: []
    volumes: []
  networkPolicy:
    enabled: false
  podSecurityPolicy:
    enabled: false
  prometheus-node-exporter:
    affinity: {}
    configmaps: []
    containerSecurityContext:
      allowPrivilegeEscalation: false
    daemonsetAnnotations: {}
    dnsConfig: {}
    enabled: false
    endpoints: []
    env: {}
    extraArgs: []
    extraHostVolumeMounts: []
    extraInitContainers: []
    global: {}
    hostNetwork: true
    hostPID: true
    hostRootFsMount:
      enabled: true
      mountPropagation: HostToContainer
    image:
      pullPolicy: IfNotPresent
      repository: quay.io/prometheus/node-exporter
      sha: ""
      tag: ""
    imagePullSecrets: []
    livenessProbe:
      failureThreshold: 3
      httpGet:
        httpHeaders: []
        scheme: http
      initialDelaySeconds: 0
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    namespaceOverride: ""
    nodeSelector: {}
    podAnnotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    podLabels: {}
    prometheus:
      monitor:
        additionalLabels: {}
        apiVersion: ""
        basicAuth: {}
        enabled: false
        interval: ""
        jobLabel: ""
        labelLimit: 0
        labelNameLengthLimit: 0
        labelValueLengthLimit: 0
        metricRelabelings: []
        namespace: ""
        proxyUrl: ""
        relabelings: []
        sampleLimit: 0
        scheme: http
        scrapeTimeout: 10s
        selectorOverride: {}
        targetLimit: 0
        tlsConfig: {}
    rbac:
      create: true
      pspAnnotations: {}
      pspEnabled: false
    readinessProbe:
      failureThreshold: 3
      httpGet:
        httpHeaders: []
        scheme: http
      initialDelaySeconds: 0
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    releaseLabel: false
    resources: {}
    secrets: []
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    service:
      annotations:
        prometheus.io/scrape: "true"
      listenOnAllInterfaces: true
      port: 9100
      portName: metrics
      targetPort: 9100
      type: ClusterIP
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: false
      create: true
      imagePullSecrets: []
    sidecarHostVolumeMounts: []
    sidecarVolumeMount: []
    sidecars: []
    tolerations:
    - effect: NoSchedule
      operator: Exists
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
    verticalPodAutoscaler:
      controlledResources: []
      enabled: false
      maxAllowed: {}
      minAllowed: {}
  prometheus-pushgateway:
    affinity: {}
    containerSecurityContext: {}
    enabled: false
    extraArgs: []
    extraContainers: []
    extraInitContainers: []
    extraVars: []
    extraVolumeMounts: []
    extraVolumes: []
    fullnameOverride: ""
    global: {}
    image:
      pullPolicy: IfNotPresent
      repository: prom/pushgateway
      tag: ""
    imagePullSecrets: []
    ingress:
      className: ""
      enabled: false
      extraPaths: []
      path: /
      pathType: ImplementationSpecific
    liveness:
      enabled: true
      probe:
        httpGet:
          path: /-/ready
          port: 9091
        initialDelaySeconds: 10
        timeoutSeconds: 10
    nameOverride: ""
    networkPolicy: {}
    nodeSelector: {}
    persistentVolume:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enabled: false
      existingClaim: ""
      mountPath: /data
      size: 2Gi
      subPath: ""
    persistentVolumeLabels: {}
    podAnnotations: {}
    podDisruptionBudget: {}
    podLabels: {}
    priorityClassName: null
    readiness:
      enabled: true
      probe:
        httpGet:
          path: /-/ready
          port: 9091
        initialDelaySeconds: 10
        timeoutSeconds: 10
    replicaCount: 1
    resources: {}
    runAsStatefulSet: false
    securityContext:
      fsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    service:
      clusterIP: ""
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      port: 9091
      targetPort: 9091
      type: ClusterIP
    serviceAccount:
      create: true
    serviceAccountLabels: {}
    serviceAnnotations:
      prometheus.io/probe: pushgateway
    serviceLabels: {}
    serviceMonitor:
      additionalLabels: {}
      enabled: false
      honorLabels: true
      metricRelabelings: []
      namespace: monitoring
      relabelings: []
    strategy:
      type: Recreate
    tolerations: {}
    topologySpreadConstraints: []
  rbac:
    create: true
  ruleFiles: {}
  server:
    affinity: {}
    alertmanagers: []
    baseURL: ""
    configMapOverrideName: ""
    configPath: /etc/config/prometheus.yml
    containerSecurityContext: {}
    defaultFlagsOverride: []
    deploymentAnnotations: {}
    dnsConfig: {}
    dnsPolicy: ClusterFirst
    emptyDir:
      sizeLimit: ""
    enableServiceLinks: true
    env: []
    extraArgs: {}
    extraConfigmapLabels: {}
    extraConfigmapMounts: []
    extraFlags:
    - web.enable-lifecycle
    extraHostPathMounts: []
    extraInitContainers: []
    extraSecretMounts: []
    extraVolumeMounts: []
    extraVolumes: []
    global:
      evaluation_interval: 30s
      scrape_interval: 5s
      scrape_timeout: 3s
    hostAliases: []
    hostNetwork: false
    image:
      pullPolicy: IfNotPresent
      repository: quay.io/prometheus/prometheus
      tag: ""
    ingress:
      annotations: {}
      enabled: false
      extraLabels: {}
      extraPaths: []
      hosts: []
      path: /
      pathType: Prefix
      tls: []
    livenessProbeFailureThreshold: 3
    livenessProbeInitialDelay: 30
    livenessProbePeriodSeconds: 15
    livenessProbeSuccessThreshold: 1
    livenessProbeTimeout: 10
    name: server
    nodeSelector: {}
    persistentVolume:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enabled: true
      existingClaim: ""
      labels: {}
      mountPath: /data
      size: 8Gi
      subPath: ""
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      maxUnavailable: 1
    podLabels: {}
    podSecurityPolicy:
      annotations: {}
    prefixURL: ""
    priorityClassName: ""
    probeHeaders: []
    probeScheme: HTTP
    readinessProbeFailureThreshold: 3
    readinessProbeInitialDelay: 30
    readinessProbePeriodSeconds: 5
    readinessProbeSuccessThreshold: 1
    readinessProbeTimeout: 4
    remoteRead: []
    remoteWrite: []
    replicaCount: 1
    resources: {}
    retention: 15d
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    service:
      annotations: {}
      clusterIP: ""
      enabled: true
      externalIPs: []
      gRPC:
        enabled: false
        servicePort: 10901
      labels: {}
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      servicePort: 9090
      sessionAffinity: None
      statefulsetReplica:
        enabled: false
        replica: 0
      type: ClusterIP
    sidecarContainers: {}
    sidecarTemplateValues: {}
    startupProbe:
      enabled: false
      failureThreshold: 30
      periodSeconds: 5
      timeoutSeconds: 10
    statefulSet:
      annotations: {}
      enabled: false
      headless:
        annotations: {}
        gRPC:
          enabled: false
          servicePort: 10901
        labels: {}
        servicePort: 80
      labels: {}
      podManagementPolicy: OrderedReady
    storagePath: ""
    strategy:
      type: Recreate
    tcpSocketProbeEnabled: false
    terminationGracePeriodSeconds: 300
    tolerations: []
    verticalAutoscaler:
      enabled: false
  serverFiles:
    alerting_rules.yml: {}
    alerts: {}
    prometheus.yml:
      rule_files:
      - /etc/config/recording_rules.yml
      - /etc/config/alerting_rules.yml
      - /etc/config/rules
      - /etc/config/alerts
      scrape_configs:
      - job_name: opentelemetry-community-demo
        kubernetes_sd_configs:
        - namespaces:
            own_namespace: true
          role: pod
        relabel_configs:
        - action: keep
          regex: true
          source_labels:
          - __meta_kubernetes_pod_annotation_opentelemetry_community_demo
    recording_rules.yml: {}
    rules: {}
  serviceAccounts:
    server:
      annotations: {}
      create: true
serviceAccount: ""
